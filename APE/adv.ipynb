{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b00ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.fft as fft\n",
    "\n",
    "class FrequencyPixelAttacks(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch transform to apply various frequency-based and pixel-based attacks.\n",
    "    \"\"\"\n",
    "    def __init__(self, attack_type='phase', epsilon=0.1, frequency_radius=0.1, num_pixels=100, noise_std=0.05, seed=None):\n",
    "        super(FrequencyPixelAttacks, self).__init__()\n",
    "        self.attack_type = attack_type\n",
    "        self.epsilon = epsilon\n",
    "        self.frequency_radius = frequency_radius\n",
    "        self.num_pixels = num_pixels\n",
    "        self.noise_std = noise_std\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        perturbed_img = img.clone().detach()\n",
    "        if self.attack_type == 'phase':\n",
    "            perturbed_img = self._phase_attack(perturbed_img)\n",
    "        elif self.attack_type == 'magnitude':\n",
    "            perturbed_img = self._magnitude_attack(perturbed_img)\n",
    "        elif self.attack_type == 'low_freq':\n",
    "            perturbed_img = self._low_frequency_attack(perturbed_img)\n",
    "        elif self.attack_type == 'high_freq':\n",
    "            perturbed_img = self._high_frequency_attack(perturbed_img)\n",
    "        elif self.attack_type == 'pixel':\n",
    "            perturbed_img = self._pixel_attack(perturbed_img)\n",
    "        elif self.attack_type == 'normal':\n",
    "            perturbed_img = self._normal_noise_attack(perturbed_img)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown attack type: {self.attack_type}\")\n",
    "        return torch.clamp(perturbed_img, 0, 1)\n",
    "    \n",
    "    def _phase_attack(self, img):\n",
    "        batch, channels, height, width = img.shape\n",
    "        perturbed_img = torch.zeros_like(img)\n",
    "        for c in range(channels):\n",
    "            f_transform = fft.fftshift(fft.fft2(img[:, c]))\n",
    "            magnitude = torch.abs(f_transform)\n",
    "            phase = torch.angle(f_transform)\n",
    "            phase_noise = torch.randn_like(phase) * self.epsilon\n",
    "            perturbed_phase = phase + phase_noise\n",
    "            f_transform_perturbed = magnitude * torch.exp(1j * perturbed_phase)\n",
    "            img_perturbed = fft.ifft2(fft.ifftshift(f_transform_perturbed)).real\n",
    "            perturbed_img[:, c] = img_perturbed\n",
    "        return perturbed_img\n",
    "    \n",
    "    def _magnitude_attack(self, img):\n",
    "        batch, channels, height, width = img.shape\n",
    "        perturbed_img = torch.zeros_like(img)\n",
    "        for c in range(channels):\n",
    "            f_transform = fft.fftshift(fft.fft2(img[:, c]))\n",
    "            magnitude = torch.abs(f_transform)\n",
    "            phase = torch.angle(f_transform)\n",
    "            magnitude_noise = torch.randn_like(magnitude) * self.epsilon * magnitude\n",
    "            perturbed_magnitude = magnitude + magnitude_noise\n",
    "            f_transform_perturbed = perturbed_magnitude * torch.exp(1j * phase)\n",
    "            img_perturbed = fft.ifft2(fft.ifftshift(f_transform_perturbed)).real\n",
    "            perturbed_img[:, c] = img_perturbed\n",
    "        return perturbed_img\n",
    "    \n",
    "    def _create_frequency_mask(self, height, width, is_low_freq=True):\n",
    "        y_indices, x_indices = torch.meshgrid(torch.arange(height), torch.arange(width), indexing='ij')\n",
    "        y_indices = y_indices - height // 2\n",
    "        x_indices = x_indices - width // 2\n",
    "        distance = torch.sqrt(y_indices**2 + x_indices**2).float()\n",
    "        max_distance = torch.sqrt(torch.tensor(height**2 + width**2, dtype=torch.float32))\n",
    "        distance /= max_distance\n",
    "        return (distance <= self.frequency_radius).float() if is_low_freq else (distance >= (1 - self.frequency_radius)).float()\n",
    "    \n",
    "    def _low_frequency_attack(self, img):\n",
    "        batch, channels, height, width = img.shape\n",
    "        perturbed_img = torch.zeros_like(img)\n",
    "        mask = self._create_frequency_mask(height, width, is_low_freq=True).to(img.device)\n",
    "        for c in range(channels):\n",
    "            f_transform = fft.fftshift(fft.fft2(img[:, c]))\n",
    "            noise = (torch.randn_like(f_transform.real) + 1j * torch.randn_like(f_transform.imag)) * self.epsilon\n",
    "            f_transform_perturbed = f_transform + noise * mask\n",
    "            img_perturbed = fft.ifft2(fft.ifftshift(f_transform_perturbed)).real\n",
    "            perturbed_img[:, c] = img_perturbed\n",
    "        return perturbed_img\n",
    "    \n",
    "    def _high_frequency_attack(self, img):\n",
    "        batch, channels, height, width = img.shape\n",
    "        perturbed_img = torch.zeros_like(img)\n",
    "        mask = self._create_frequency_mask(height, width, is_low_freq=False).to(img.device)\n",
    "        for c in range(channels):\n",
    "            f_transform = fft.fftshift(fft.fft2(img[:, c]))\n",
    "            noise = (torch.randn_like(f_transform.real) + 1j * torch.randn_like(f_transform.imag)) * self.epsilon\n",
    "            f_transform_perturbed = f_transform + noise * mask\n",
    "            img_perturbed = fft.ifft2(fft.ifftshift(f_transform_perturbed)).real\n",
    "            perturbed_img[:, c] = img_perturbed\n",
    "        return perturbed_img\n",
    "    \n",
    "    def _pixel_attack(self, img):\n",
    "        batch, channels, height, width = img.shape\n",
    "        perturbed_img = img.clone()\n",
    "        num_pixels = min(self.num_pixels, height * width)\n",
    "        pixel_indices = torch.randint(0, height * width, (batch, num_pixels))\n",
    "        y_indices, x_indices = pixel_indices // width, pixel_indices % width\n",
    "        for b in range(batch):\n",
    "            for c in range(channels):\n",
    "                for i in range(num_pixels):\n",
    "                    perturbed_img[b, c, y_indices[b, i], x_indices[b, i]] += torch.randn(1).item() * self.epsilon\n",
    "        return perturbed_img\n",
    "    \n",
    "    def _normal_noise_attack(self, img):\n",
    "        noise = torch.randn_like(img) * self.noise_std\n",
    "        return img + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9839bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "def display_image(original, adversarial):\n",
    "    \"\"\"Display last batch of original and adversarial images.\"\"\"\n",
    "    fig, axes = plt.subplots(2, len(original), figsize=(16, 10))\n",
    "    \n",
    "    for i in range(len(original)):\n",
    "        orig_img = original[i].permute(1, 2, 0).cpu().numpy()\n",
    "        adv_img = adversarial[i].permute(1, 2, 0).cpu().numpy()\n",
    "        \n",
    "        orig_img = np.clip(orig_img, 0, 1)\n",
    "        adv_img = np.clip(adv_img, 0, 1)\n",
    "\n",
    "        \n",
    "        axes[0, i].imshow(orig_img)\n",
    "        axes[0, i].axis(\"off\")\n",
    "\n",
    "        axes[1, i].imshow(adv_img)\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def train_adversary(model,test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 1. Freeze the model (so only adversary parameters are updated)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.eval()\n",
    "    \n",
    "    # 2. Evaluate adversarial attack on test data\n",
    "    # test_loader = load_test(batch_size=1)  # Load test dataset\n",
    "    test_dataset = test_loader.dataset  # Get dataset without batching\n",
    "\n",
    "    attacks = [\n",
    "        # {'attack_type': 'phase', 'epsilon': 0.1, 'frequency_radius': 0.1, 'num_pixels': 100, 'noise_std': 0.05, 'name': 'phase_weak'},\n",
    "        {'attack_type': 'phase', 'epsilon': 0.5, 'frequency_radius': 0.1, 'num_pixels': 100, 'noise_std': 0.05, 'name': 'phase_strong'},\n",
    "        # {'attack_type': 'magnitude', 'epsilon': 0.1, 'frequency_radius': 0.1, 'num_pixels': 100, 'noise_std': 0.05, 'name': 'magnitude_weak'},\n",
    "        {'attack_type': 'magnitude', 'epsilon': 0.5, 'frequency_radius': 0.1, 'num_pixels': 100, 'noise_std': 0.05, 'name': 'magnitude_strong'},\n",
    "        # {'attack_type': 'low_freq', 'epsilon': 0.2, 'frequency_radius': 0.1, 'num_pixels': 100, 'noise_std': 0.05, 'name': 'low_freq_small_radius'},\n",
    "        # {'attack_type': 'low_freq', 'epsilon': 0.2, 'frequency_radius': 0.3, 'num_pixels': 100, 'noise_std': 0.05, 'name': 'low_freq_large_radius'},\n",
    "        # {'attack_type': 'high_freq', 'epsilon': 0.2, 'frequency_radius': 0.1, 'num_pixels': 100, 'noise_std': 0.05, 'name': 'high_freq_small_radius'},\n",
    "        # {'attack_type': 'high_freq', 'epsilon': 0.2, 'frequency_radius': 0.3, 'num_pixels': 100, 'noise_std': 0.05, 'name': 'high_freq_large_radius'},\n",
    "        # {'attack_type': 'pixel', 'epsilon': 0.5, 'frequency_radius': 0.1, 'num_pixels': 100, 'noise_std': 0.05, 'name': 'pixel_few'},\n",
    "        # {'attack_type': 'pixel', 'epsilon': 0.5, 'frequency_radius': 0.1, 'num_pixels': 1000, 'noise_std': 0.05, 'name': 'pixel_many'},\n",
    "        # {'attack_type': 'normal', 'epsilon': 0.1,  'frequency_radius': 0.1, 'num_pixels': 100, 'noise_std': 0.05, 'name': 'normal_weak'},\n",
    "        # {'attack_type': 'normal', 'epsilon': 0.1, 'frequency_radius': 0.1, 'num_pixels': 100, 'noise_std': 0.2, 'name': 'normal_strong'}\n",
    "    ]\n",
    "\n",
    "    for attack in attacks:\n",
    "        total_correct_original = 0\n",
    "        total_correct_perturbed = 0\n",
    "        total_samples = 0\n",
    "        last_batch_images = []\n",
    "        last_batch_perturbed = []\n",
    "        \n",
    "        adversary = FrequencyPixelAttacks(attack[\"attack_type\"], attack[\"epsilon\"], attack[\"frequency_radius\"], attack[\"num_pixels\"], attack[\"noise_std\"])\n",
    "\n",
    "        # Process each image individually\n",
    "        for i in range(len(test_dataset)):\n",
    "            image, label = test_dataset[i]  # Get single image and label\n",
    "            image, label = image.to(device).unsqueeze(0), torch.tensor([label]).to(device)  # Add batch dimension\n",
    "\n",
    "            # image = image.squeeze(0)  # Removes the batch dimension\n",
    "            perturbed_image = adversary(image)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # y_hat = model(image)\n",
    "                y_pred = model(perturbed_image)\n",
    "            \n",
    "            # pred_original = torch.argmax(y_hat, dim=1)\n",
    "            pred_perturbed = torch.argmax(y_pred, dim=1)\n",
    "            \n",
    "            # total_correct_original += (pred_original == label).item()\n",
    "            total_correct_perturbed += (pred_perturbed == label).item()\n",
    "            total_samples += 1  # Each image is processed individually\n",
    "            \n",
    "            if i < 5:  # Store only a few images for visualization\n",
    "                # last_batch_images.append(image.cpu().squeeze(0))\n",
    "                last_batch_perturbed.append(perturbed_image.cpu().squeeze(0))\n",
    "\n",
    "        # original_acc = (total_correct_original / total_samples) * 100\n",
    "        perturbed_acc = (total_correct_perturbed / total_samples) * 100\n",
    "    \n",
    "        print(f\"Attack: {attack['name']}\")\n",
    "        print(\"Total Samples: \", total_samples)\n",
    "        # print(f\"Accuracy on Original Images: {original_acc:.2f}%\")\n",
    "        print(f\"Accuracy on Perturbed Images: {perturbed_acc:.2f}%\")\n",
    "\n",
    "        # display_image(last_batch_images, last_batch_perturbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f946448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adversarial-robustness-toolbox in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (1.19.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.6.1)\n",
      "Requirement already satisfied: six in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (75.8.0)\n",
      "Requirement already satisfied: tqdm in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (4.67.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (8.1.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: decorator in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/aarsh.wankar/miniconda/envs/ml/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install adversarial-robustness-toolbox\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b003e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent\n",
    "from art.attacks.evasion import CarliniL2Method\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Utility function for visualization ---\n",
    "def display_image(original, adversarial, title=\"\"):\n",
    "    \"\"\"Display a batch of original and adversarial images.\"\"\"\n",
    "    n = len(original)\n",
    "    fig, axes = plt.subplots(2, n, figsize=(4 * n, 8))\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Convert tensor to numpy (assumes tensor shape is [C, H, W])\n",
    "        orig_img = original[i].permute(1, 2, 0).cpu().numpy()\n",
    "        adv_img = adversarial[i].permute(1, 2, 0).cpu().numpy()\n",
    "        \n",
    "        orig_img = np.clip(orig_img, 0, 1)\n",
    "        adv_img = np.clip(adv_img, 0, 1)\n",
    "        \n",
    "        axes[0, i].imshow(orig_img)\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        \n",
    "        axes[1, i].imshow(adv_img)\n",
    "        axes[1, i].set_title(\"Adversarial\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Modified training function using ART attacks ---\n",
    "def train_adversary_art(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # IMPORTANT: Freeze model parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Wrap the ViT model into an ART classifier.\n",
    "    # Adjust input_shape and nb_classes according to your ViT model.\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Not used for attacks.\n",
    "    classifier = PyTorchClassifier(\n",
    "        model=model,\n",
    "        loss=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        input_shape=(3, 224, 224),  # Example shape for ViT (channels, height, width)\n",
    "        nb_classes=10,           # Update based on your model's number of classes\n",
    "        clip_values=(0, 1)\n",
    "    )\n",
    "\n",
    "    # Define ART attack objects with different parameters\n",
    "    attacks = [\n",
    "        {\"name\": \"FGSM_weak\", \"attack\": FastGradientMethod(estimator=classifier, eps=0.1)},\n",
    "        {\"name\": \"FGSM_strong\", \"attack\": FastGradientMethod(estimator=classifier, eps=0.5)},\n",
    "        {\"name\": \"PGD\", \"attack\": ProjectedGradientDescent(estimator=classifier, eps=0.3, max_iter=10)},\n",
    "        # {\"name\": \"CW\", \"attack\": CarliniL2Method(classifier=classifier, max_iter=3)}\n",
    "    ]\n",
    "\n",
    "    # Loop over each attack type\n",
    "    for attack_dict in attacks:\n",
    "        attack_name = attack_dict[\"name\"]\n",
    "        attack_obj = attack_dict[\"attack\"]\n",
    "        total_correct_original = 0\n",
    "        total_correct_adv = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # To store a few examples for visualization\n",
    "        orig_examples = []\n",
    "        adv_examples = []\n",
    "\n",
    "        # Iterate over the test dataset (assumes test_loader returns (image, label))\n",
    "        for i, (image, label) in enumerate(test_loader):\n",
    "            image = image.to(device)  # image shape: [B, C, H, W]\n",
    "            label = label.to(device)\n",
    "\n",
    "            # ART expects numpy arrays. Convert and ensure the shape is [B, C, H, W]\n",
    "            image_np = image.cpu().numpy()\n",
    "            \n",
    "            # Generate adversarial examples using ART\n",
    "            adv_image_np = attack_obj.generate(x=image_np)\n",
    "            adv_image = torch.from_numpy(adv_image_np).to(device)\n",
    "            print(\"here\")\n",
    "            # Get predictions for original and adversarial images\n",
    "            with torch.no_grad():\n",
    "                output_orig = model(image)\n",
    "                output_adv = model(adv_image)\n",
    "            \n",
    "            pred_orig = output_orig.argmax(dim=1)\n",
    "            pred_adv = output_adv.argmax(dim=1)\n",
    "            total_correct_original += (pred_orig == label).sum().item()\n",
    "            total_correct_adv += (pred_adv == label).sum().item()\n",
    "            total_samples += image.shape[0]\n",
    "\n",
    "            # Save the first batch (or a few images) for visualization\n",
    "            if len(orig_examples) < 5:\n",
    "                orig_examples.append(image[0].cpu())\n",
    "                adv_examples.append(adv_image[0].cpu())\n",
    "            \n",
    "            del image, label, adv_image, output_orig, output_adv\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        original_acc = (total_correct_original / total_samples) * 100\n",
    "        adv_acc = (total_correct_adv / total_samples) * 100\n",
    "        \n",
    "        print(f\"Attack: {attack_name}\")\n",
    "        print(\"Total Samples: \", total_samples)\n",
    "        print(f\"Accuracy on Original Images: {original_acc:.2f}%\")\n",
    "        print(f\"Accuracy on Adversarial Images: {adv_acc:.2f}%\")\n",
    "        # display_image(orig_examples, adv_examples)\n",
    "\n",
    "# --- Example usage ---\n",
    "# Assuming you have a pre-trained ViT model and a test_loader defined.\n",
    "# model = ...  # Your ViT model (e.g. from timm or torchvision.models)\n",
    "# test_loader = ...  # Your DataLoader for the test set\n",
    "# train_adversary_art(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7cae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.models.image.cct import AlgebraicCCT, SinusoidalCCT, AbsoluteCCT, AlgebraicSeqCCT,ourCCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b9b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17e4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Oxford-IIIT Pet training data\n",
    "def load_train(batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "        # transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                      std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def load_test(batch_size=16):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7385b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62557e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8301e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from eval.models.image.augmentations import CIFAR10Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edf75839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model... Loading the model\n",
      "Attack: phase_strong\n",
      "Total Samples:  10000\n",
      "Accuracy on Perturbed Images: 54.75%\n",
      "Attack: magnitude_strong\n",
      "Total Samples:  10000\n",
      "Accuracy on Perturbed Images: 54.32%\n"
     ]
    }
   ],
   "source": [
    "dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 7\n",
    "mlp_ratio = 2\n",
    "in_channels, num_classes, image_size = 3, 10, 10\n",
    "\n",
    "augmentations = [CIFAR10Policy(),\n",
    "                    transforms.RandomCrop((32, 32), padding=4),\n",
    "                    transforms.RandomHorizontalFlip()]\n",
    "transformations = [transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]\n",
    "train_set = CIFAR10(\"data\" ,train=True, download=True,\n",
    "                    transform=transforms.Compose([*augmentations, *transformations]))\n",
    "test_set = CIFAR10(\"data\", train=False, download=True,\n",
    "                    transform=transforms.Compose(transformations))\n",
    "in_channels, num_classes, image_size = 3, 10, 10\n",
    "\n",
    "train_dl = DataLoader(train_set, batch_size = 1, shuffle=True, num_workers=4)\n",
    "test_dl = DataLoader(test_set, batch_size= 1, shuffle=False, num_workers=4)\n",
    "\n",
    "model = AlgebraicCCT(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                num_layers=num_layers,\n",
    "                kernel_size=(4, 4),\n",
    "                in_channels=in_channels,\n",
    "                num_classes=num_classes,\n",
    "                mlp_ratio=mlp_ratio).cuda()\n",
    "model_path = \"best_model_.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Found model... Loading the model\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# train_adversary_art(model, test_loader)\n",
    "train_adversary(model,test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3250b7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model... Loading the model\n",
      "Attack: phase_strong\n",
      "Total Samples:  10000\n",
      "Accuracy on Perturbed Images: 53.88%\n",
      "Attack: magnitude_strong\n",
      "Total Samples:  10000\n",
      "Accuracy on Perturbed Images: 53.68%\n"
     ]
    }
   ],
   "source": [
    "dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 7\n",
    "mlp_ratio = 2\n",
    "in_channels, num_classes, image_size = 3, 10, 10\n",
    "\n",
    "augmentations = [CIFAR10Policy(),\n",
    "                    transforms.RandomCrop((32, 32), padding=4),\n",
    "                    transforms.RandomHorizontalFlip()]\n",
    "transformations = [transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]\n",
    "train_set = CIFAR10(\"data\" ,train=True, download=True,\n",
    "                    transform=transforms.Compose([*augmentations, *transformations]))\n",
    "test_set = CIFAR10(\"data\", train=False, download=True,\n",
    "                    transform=transforms.Compose(transformations))\n",
    "in_channels, num_classes, image_size = 3, 10, 10\n",
    "\n",
    "train_dl = DataLoader(train_set, batch_size = 1, shuffle=True, num_workers=4)\n",
    "test_dl = DataLoader(test_set, batch_size= 1, shuffle=False, num_workers=4)\n",
    "\n",
    "model = ourCCT(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                num_layers=num_layers,\n",
    "                kernel_size=(4, 4),\n",
    "                in_channels=in_channels,\n",
    "                num_classes=num_classes,\n",
    "                mlp_ratio=mlp_ratio).cuda()\n",
    "model_path = \"best_model_t.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Found model... Loading the model\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# train_adversary_art(model, test_loader)\n",
    "train_adversary(model,test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b8ccc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model... Loading the model\n",
      "Attack: phase_strong\n",
      "Total Samples:  10000\n",
      "Accuracy on Perturbed Images: 52.59%\n",
      "Attack: magnitude_strong\n",
      "Total Samples:  10000\n",
      "Accuracy on Perturbed Images: 52.55%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 7\n",
    "mlp_ratio = 2\n",
    "in_channels, num_classes, image_size = 3, 10, 10\n",
    "\n",
    "augmentations = [CIFAR10Policy(),\n",
    "                    transforms.RandomCrop((32, 32), padding=4),\n",
    "                    transforms.RandomHorizontalFlip()]\n",
    "transformations = [transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]\n",
    "train_set = CIFAR10(\"data\" ,train=True, download=True,\n",
    "                    transform=transforms.Compose([*augmentations, *transformations]))\n",
    "test_set = CIFAR10(\"data\", train=False, download=True,\n",
    "                    transform=transforms.Compose(transformations))\n",
    "in_channels, num_classes, image_size = 3, 10, 10\n",
    "\n",
    "train_dl = DataLoader(train_set, batch_size = 1, shuffle=True, num_workers=4)\n",
    "test_dl = DataLoader(test_set, batch_size= 1, shuffle=False, num_workers=4)\n",
    "\n",
    "model = SinusoidalCCT(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                num_layers=num_layers,\n",
    "                kernel_size=(4, 4),\n",
    "                in_channels=in_channels,\n",
    "                num_classes=num_classes,\n",
    "                mlp_ratio=mlp_ratio).cuda()\n",
    "model_path = \"best_model_s.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Found model... Loading the model\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# train_adversary_art(model, test_loader)\n",
    "train_adversary(model,test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "945f6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 7\n",
    "mlp_ratio = 2\n",
    "in_channels, num_classes, image_size = 3, 10, 10\n",
    "\n",
    "augmentations = [CIFAR10Policy(),\n",
    "                    transforms.RandomCrop((32, 32), padding=4),\n",
    "                    transforms.RandomHorizontalFlip()]\n",
    "transformations = [transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]\n",
    "train_set = CIFAR10(\"data\" ,train=True, download=True,\n",
    "                    transform=transforms.Compose([*augmentations, *transformations]))\n",
    "test_set = CIFAR10(\"data\", train=False, download=True,\n",
    "                    transform=transforms.Compose(transformations))\n",
    "in_channels, num_classes, image_size = 3, 10, 10\n",
    "\n",
    "train_dl = DataLoader(train_set, batch_size = 1, shuffle=True, num_workers=4)\n",
    "test_dl = DataLoader(test_set, batch_size= 1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec15416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model... Loading the model\n"
     ]
    }
   ],
   "source": [
    "dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 7\n",
    "mlp_ratio = 2\n",
    "in_channels, num_classes, image_size = 3, 10, 10\n",
    "\n",
    "augmentations = [CIFAR10Policy(),\n",
    "                    transforms.RandomCrop((32, 32), padding=4),\n",
    "                    transforms.RandomHorizontalFlip()]\n",
    "transformations = [transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]\n",
    "train_set = CIFAR10(\"data\" ,train=True, download=True,\n",
    "                    transform=transforms.Compose([*augmentations, *transformations]))\n",
    "test_set = CIFAR10(\"data\", train=False, download=True,\n",
    "                    transform=transforms.Compose(transformations))\n",
    "in_channels, num_classes, image_size = 3, 10, 10\n",
    "\n",
    "train_dl = DataLoader(train_set, batch_size = 1, shuffle=True, num_workers=4)\n",
    "test_dl = DataLoader(test_set, batch_size= 1, shuffle=False, num_workers=4)\n",
    "\n",
    "model = AlgebraicCCT(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                num_layers=num_layers,\n",
    "                kernel_size=(4, 4),\n",
    "                in_channels=in_channels,\n",
    "                num_classes=num_classes,\n",
    "                mlp_ratio=mlp_ratio).cuda()\n",
    "model_path = \"best_model_.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Found model... Loading the model\")\n",
    "    model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19b78dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor shape: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Get one batch from the training dataloader\n",
    "images, labels = next(iter(train_dl))\n",
    "\n",
    "# Print the shape of the image tensor\n",
    "print(\"Image tensor shape:\", images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12aa510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.52%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total * 100\n",
    "\n",
    "# Calculate accuracy\n",
    "# train_acc = evaluate_accuracy(model, train_dl)\n",
    "test_acc = evaluate_accuracy(model, test_dl)\n",
    "\n",
    "# print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74522b",
   "metadata": {},
   "source": [
    "# Frequency Attacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce477b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fft\n",
    "from torch.fft import fft2, ifft2, fftshift, ifftshift\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eba12419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def fourier_attack(model, image, label, epsilon=0.1, lambda_reg=0.1, num_iters=10):\n",
    "    \"\"\"\n",
    "    Implements the Fourier-based adversarial attack.\n",
    "    :param model: Target classifier\n",
    "    :param image: Input image (normalized, tensor of shape [1, C, H, W])\n",
    "    :param label: Ground truth label (integer tensor)\n",
    "    :param epsilon: Perturbation strength\n",
    "    :param lambda_reg: Regularization factor balancing L2 and CE loss\n",
    "    :param num_iters: Number of attack iterations\n",
    "    :return: Adversarial image\n",
    "    \"\"\"\n",
    "    # Ensure the image requires gradients\n",
    "    image = image.clone().detach().requires_grad_(True)\n",
    "    label = label.to(image.device)\n",
    "    \n",
    "    # Compute Fourier Transform of the image\n",
    "    fft_image = fft2(image)\n",
    "    fft_image = fftshift(fft_image)  # Shift low frequencies to center\n",
    "    magnitude = torch.abs(fft_image)\n",
    "    phase = torch.angle(fft_image)\n",
    "    \n",
    "    # Initialize perturbations\n",
    "    delta_mag = torch.ones_like(magnitude, requires_grad=True, device=image.device)\n",
    "    delta_phase = torch.zeros_like(phase, requires_grad=True, device=image.device)\n",
    "    delta_pixel = torch.zeros_like(image, requires_grad=True, device=image.device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([delta_mag, delta_phase, delta_pixel], lr=0.01)\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Apply perturbations\n",
    "        perturbed_magnitude = magnitude * delta_mag\n",
    "        perturbed_phase = phase + delta_phase\n",
    "        \n",
    "        # Construct perturbed Fourier representation\n",
    "        perturbed_fft = perturbed_magnitude * torch.exp(1j * perturbed_phase)\n",
    "        perturbed_fft = ifftshift(perturbed_fft)  # Shift back before ifft2\n",
    "        perturbed_image = ifft2(perturbed_fft).real + delta_pixel  # Ensure real values\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)  # Clip to valid range\n",
    "        \n",
    "        # Compute loss\n",
    "        l2_loss = F.mse_loss(perturbed_image, image)\n",
    "        ce_loss = F.cross_entropy(model(perturbed_image), label)\n",
    "        loss = lambda_reg * l2_loss - ce_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Clip perturbations\n",
    "        delta_mag.data = torch.clamp(delta_mag, 1 - epsilon, 1 + epsilon)\n",
    "        delta_phase.data = torch.clamp(delta_phase, -epsilon, epsilon)\n",
    "        delta_pixel.data = torch.clamp(delta_pixel, -epsilon, epsilon)\n",
    "    \n",
    "    return perturbed_image.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62fc5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_attack_and_evaluate(model, data_loader, attack):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    for inputs, targets in tqdm(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        perturbed_inputs = attack(model,inputs,label=targets)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(perturbed_inputs)\n",
    "            # print(outputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9e116bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# Create a subset with the first 100 samples\n",
    "subset_indices = list(range(1000))\n",
    "test_subset = Subset(test_dl.dataset, subset_indices)\n",
    "\n",
    "# Create a new DataLoader with this subset\n",
    "test_100_dl = DataLoader(test_subset, batch_size=1, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad0d888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model... Loading the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarsh.wankar/APE_FOLDER/ape_local/nn/position/algebraic.py:69: RuntimeWarning: logm result may be inaccurate, approximate err = 2.608585987218248e-07\n",
      "  log = torch.tensor(logm(out)).real\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model... Loading the model\n",
      "Found model... Loading the model\n"
     ]
    }
   ],
   "source": [
    "model = SinusoidalCCT(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                num_layers=num_layers,\n",
    "                kernel_size=(4, 4),\n",
    "                in_channels=in_channels,\n",
    "                num_classes=num_classes,\n",
    "                mlp_ratio=mlp_ratio).cuda()\n",
    "model_path = \"best_model_s.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Found model... Loading the model\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model1 = AlgebraicCCT(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            kernel_size=(4, 4),\n",
    "            in_channels=in_channels,\n",
    "            num_classes=num_classes,\n",
    "            mlp_ratio=mlp_ratio).cuda()\n",
    "model_path = \"best_model_.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Found model... Loading the model\")\n",
    "    model1.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model2 = ourCCT(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            kernel_size=(4, 4),\n",
    "            in_channels=in_channels,\n",
    "            num_classes=num_classes,\n",
    "            mlp_ratio=mlp_ratio).cuda()\n",
    "model_path = \"best_model_t.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Found model... Loading the model\")\n",
    "    model2.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "994e90ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:39<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal:  3.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Sinusoidal: ' ,apply_attack_and_evaluate(model,test_100_dl,fourier_attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9c06f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:40<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APE:  2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('APE: ' ,apply_attack_and_evaluate(model1,test_100_dl,fourier_attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1fe721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:02<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ours:  4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Ours: ' ,apply_attack_and_evaluate(model2,test_100_dl,fourier_attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1892f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
